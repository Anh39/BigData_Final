{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "import torch\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data():\n",
    "    vocab = get_vocab()\n",
    "    word2idx = {w:i for i,w in enumerate(vocab)}\n",
    "    idx2word = {i:w for i,w in enumerate(vocab)}\n",
    "    labels = []\n",
    "    with open(\"data/ag_news_data/train.csv\", 'r') as file:\n",
    "        lines = file.readlines()[1:]\n",
    "        for line in lines:\n",
    "            labels.append(int(line.split(\",\")[-2]))\n",
    "        lines = [\",\".join(line.split(\",\")[:-2]) for line in lines]\n",
    "    train_data = []\n",
    "    for i in range(len(lines)):\n",
    "        lines[i] = extract(lines[i])[:SEQ_LENGTH]\n",
    "        if len(lines[i]) < SEQ_LENGTH:\n",
    "            lines[i].extend(['<pad>' for _ in range(SEQ_LENGTH - len(lines[i]))])\n",
    "        label = labels[i]\n",
    "        inputs = [word2idx[w] for w in lines[i]]\n",
    "        targets = [1 if i == label else 0 for i in range(OUTPUT_SIZE)]\n",
    "        train_data.append([inputs, label])\n",
    "    return train_data, vocab\n",
    "train_data, vocab = load_train_data()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedd = torch.nn.Embedding(\n",
    "            num_embeddings=len(vocab),\n",
    "            embedding_dim=EMBED_DIM\n",
    "        )\n",
    "        self.fc1 = torch.nn.Linear(EMBED_DIM*SEQ_LENGTH, HIDDEN_SIZE)\n",
    "        self.fc2 = torch.nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "        self.acf1 = torch.nn.Sigmoid()\n",
    "        self.acf2 = torch.nn.Softmax()\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.embedd(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.acf1(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4263815879821777\n",
      "Accuracy : 0.39\n",
      "3.374135971069336\n",
      "Accuracy : 0.4254\n",
      "1.7485531568527222\n",
      "Accuracy : 0.4652\n",
      "1.3207236528396606\n",
      "Accuracy : 0.4362\n",
      "1.55916428565979\n",
      "Accuracy : 0.4508\n",
      "1.4312894344329834\n",
      "Accuracy : 0.5312\n",
      "1.135550856590271\n",
      "Accuracy : 0.623\n",
      "0.9274903535842896\n",
      "Accuracy : 0.6836\n",
      "0.8350181579589844\n",
      "Accuracy : 0.6916\n",
      "0.807162880897522\n",
      "Accuracy : 0.6718\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier()\n",
    "train_data = train_data[:len(train_data)//24]\n",
    "batch_size = len(train_data) \n",
    "n_batch = len(train_data)//batch_size\n",
    "lossf = torch.nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "n_epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-2)\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(n_batch):\n",
    "        optimizer.zero_grad()\n",
    "        batch_data = train_data[i*batch_size:(i+1)*batch_size]\n",
    "        inputs = torch.tensor([batch_data[i][0] for i in range(batch_size)]).to(device)\n",
    "        targets = torch.tensor([batch_data[i][1] for i in range(batch_size)]).long().to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = lossf(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(total_loss)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in range(n_batch):\n",
    "            optimizer.zero_grad()\n",
    "            batch_data = train_data[i*batch_size:(i+1)*batch_size]\n",
    "            inputs = torch.tensor([batch_data[i][0] for i in range(batch_size)]).to(device)\n",
    "            targets = torch.tensor([batch_data[i][1] for i in range(batch_size)]).long().to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = lossf(outputs, targets)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "        print(f\"Accuracy : {correct/total}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
